from langchain.chat_models import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from langchain.prompts import ChatPromptTemplate
from typing import List
import os
import dotenv

dotenv.load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ì˜ˆ: OpenAI ë˜ëŠ” Bedrock)
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
# os.environ["AWS_ACCESS_KEY_ID"] = os.getenv("AWS_ACCESS_KEY_ID")
# os.environ["AWS_SECRET_ACCESS_KEY"] = os.getenv("AWS_SECRET_ACCESS_KEY")

# ğŸ”¹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¶ˆëŸ¬ì˜¤ê¸°
def load_prompt_template(file_path: str) -> ChatPromptTemplate:
    with open(file_path, encoding="utf-8") as f:
        template_str = f.read()
    return ChatPromptTemplate.from_template(template_str)


# ğŸ”¹ ì§ˆë¬¸ + ë¬¸ì„œ â†’ LLM ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response_text(question_text: str, docs: List[str]) -> str:
    """
    ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    :param question_text: ìœ ì €ì˜ ì§ˆë¬¸
    :param docs: ê´€ë ¨ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    :return: LLMì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    try:
        prompt_template_path = "./prompts/policy_chat_prompt.txt"

        context = "\n".join(docs)
        prompt = load_prompt_template(prompt_template_path)
        messages = prompt.format_messages(question=question_text, context=context)

        llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0.7)
        response = llm(messages)

        return response.content
    except Exception as e:
        raise RuntimeError(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")